{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Problem Formulation\n","\n","In text classification, we aim to assign a set of pre-defined classes to documents. Below are two examples from the dataset [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/):\n","<br/><br/>\n","\n","Document                            | Class \n","---------------------------------------|---------------\n","The radio telescope at arecibo observatory will begin mapping the known galaxy on Friday, scientists said. | Science\n","The world record holder was in fourth place at the time he was losing ground. | Sports\n","\n","<br/>\n","\n","A naive Bayes classifier is a simple and classic model for text classification. Let there be \n","<ul>\n","  <li>$N$ pre-defined classes $\\{c_1,\\dots,c_N\\}$ and </li>\n","  <li>a vocabulary $V=\\{w_1, w_2, \\dots, w_{|V|}\\}$ of size $|V|$, where $w_k$ is some word.</li>\n","</ul>\n","\n","In our task, our data will be text documents. Here a document is defined as a sequence of words. <br/>\n","The training dataset is $(X,Y)$, where \n","<ul>\n","  <li>$X=\\{x_1, x_2, \\dots, x_d\\}$, containing d documents, each $x_i$ is a document; </li>\n","  <li>their class labels $Y=\\{y_1, y_2, \\dots, y_d\\}$, where $y_i$ is the class label of $x_i$, $y_i\\in\\{c_1,\\dots,c_N\\}$.</li>\n","</ul>\n","<br/>\n","\n","***\n","\n","### <font color='blue'>Example</font> ðŸ˜‰\n","Don't get scared by these notations. You can have a quick and clear understanding through the following example.\n","\n","Suppose there are \n","<ul>\n","  <li>2 classes: $c_1=Science, c_2=Sports$. </li>\n","  <li>Let the vocabulary $V=\\{w_1,\\dots,w_{9}\\}=\\{$\"the\", \"telescope\", \"at\", \"observatory\", \"galaxy\", \"scientists\", \"record\", \"place\", \"losing\"$\\}$ with the size $|V|=9$. </li>\n","</ul>\n","\n","There are two documents in the training dataset, which are shown in the above table. That is, $X=\\{x_1,x_2\\}$, \n","\n","$x_1=$\"*The radio telescope at arecibo observatory will begin mapping the known galaxy on Friday, scientists said.*\", \n","\n","$x_2=$\"*The world record holder was in fourth place at the time he was losing ground.*\" \n","\n","Their class labels $Y=\\{y_1,y_2\\}=\\{Science, Sports\\}$. \n","<br/>\n","<br/>\n","We will use this example in the next formulation.\n","\n","***"],"metadata":{"id":"Qu2hsyTrXcMT"}},{"cell_type":"markdown","source":["## Training the Naive Bayes Text Classifier\n","<!-- <code style=\"background:blue;color:black\">This may be the first time you deal with the text classification task, but the core of the assignment is to implement the naive Bayes framework. Please note that the following formulations are direct applications of what you have learned in class.</code>  -->\n","\n","<div class=\"alert alert-block alert-info\"><b>Note:</b> This may be the first time you deal with the text classification task, but the core of the assignment is to implement the naive Bayes framework. Please note that the following formulations directly apply what you have learned in class. ðŸ˜ƒ</div>\n","\n","Learning a naive Bayes text classifier consists of estimating the parameters, including the \n","<ul>\n","  <li><b>class probabilities</b> $P(c_j)$ and the </li>\n","  <li><b>word probabilities</b> $P(w_k|c_j)$, for $j=1,\\dots,N$ and $w_k\\in V$.</li>\n","</ul>\n","\n","Based on the generative model for text and the standard naive Bayes assumption:\n","\n","> The words of a document are conditionally independent of the other words in the same document, given the class label. \n","\n","The **parameter estimates** are: (please refer to [Nigam et al., 2006] for the detailed derivation if interested.)\n","\n","$P(c_j)=\\frac{1+\\delta_{1j}+\\delta_{2j}+\\dots+\\delta_{dj}}{N+d}=\\frac{1+\\sum_{i=1}^d\\delta_{ij}}{N+d}$,\n","\n","$P(w_k|c_j)=\\frac{1+\\delta_{1j}x_{1k}+\\delta_{2j}x_{2k}+\n","\\dots+\\delta_{dj}x_{dk}}{|V|+(\\delta_{1j}x_{11}+\\delta_{2j}x_{21}+\n","\\dots+\\delta_{dj}x_{d1})+\\dots+(\\delta_{1j}x_{1|V|}+\\delta_{2j}x_{2|V|}+\n","\\dots+\\delta_{dj}x_{d|V|})}=\\frac{1+\\sum_{i=1}^d\\delta_{ij}x_{ik}}{|V|+\\sum_{s=1}^{|V|}\\sum_{i=1}^d\\delta_{ij}x_{is}}$,\n","\n","where \n","<ul>\n","  <li>$\\delta_{ij}$ is given by the class label: \n","<ul>\n","  <li>$\\delta_{ij}=1$ when $y_i=c_j$ and </li>\n","  <li>$\\delta_{ij}=0$ otherwise.</li>\n","</ul></li>\n","<li>$x_{ik}$ is the number of times word $w_k$ occurs in documents $x_i$.</li>\n","</ul>\n","<br/>\n","The formulas shown above are derived from the Naive Bayes formulas in the lecture notes. However, you may have noticed that they are slightly different with the formulas in lecture notes, since <ins>the Naive Bayes probabilities in the lecture notes exactly <b>come from the empirical event counts</b>, while in above formulas, we <b>add 1 count to each event</b></ins>. We apply the additional 1 count in order to <b>avoid zero probability in text classification</b>.\n","<br/>\n","<div class=\"alert alert-block alert-info\"><b>Note:</b> If you feel a little bit lost for now, that's ok. We provide a symbol summary list for your reference. In addition, please go through the following example to have a clear understanding. ðŸ˜ƒ</div>\n","<br/>\n","\n","| Symbol | Meaning |\n","| :--- |:--- |\n","| $c_j$ | Class |\n","| $N$ | Number of classes |\n","| $V$ | Vocabulary |\n","| $|V|$ | Vocabulary size |\n","| $w_k$ | Word in vocabulary |\n","| $d$ | Number of training documents |\n","| $X$ | Training data set |\n","| $x_i$ | Document |\n","| $Y$ | Training class label set |\n","| $y_i$ | Class label of $x_i$ |\n","| $\\delta_{ij}$ | Indicate whether $y_i=c_j$ |\n","| $x_{ik}$ | Number of times $w_k \\text{ occurs in } x_i$ |\n"," \n","***\n","\n","<br/>\n","\n","### <font color='blue'>Example</font> ðŸ˜‰\n","Recall our example. With the two training documents, we need to estimate the class probabilities <br/>\n","$P(Science)$ and $P(Sports)$; \n","<br/>\n","as well as the word probabilities <br/>\n","$P(w_k|Science)$ and $P(w_k|Sports)$ for $w_k\\in V$. \n","\n","Therefore, we have 20 parameters to estimate (2 for class probabilities and 18 for word probabilities).\n","<br/><br/>\n","First, let's calculate $\\delta_{ij}$ and $x_{ik}$. \n","\n","In our example, $y_1=Science=c_1, y_2=Sports=c_2$, so we can have $\\delta_{11}=\\delta_{22}=1, \\delta_{12}=\\delta_{21}=0$. \n","<br/><br/>\n","Next, we count the number of times each word in vocabulary $V$ occurs in documents. \n","\n","In document $x_1$, \n","- the word $w_1=$\"*the*\" occurs 2 times, so $x_{11}=2$; \n","- the word $w_2=$\"*telescope*\" occurs 1 time, so $x_{12}=1$;\n","- the word $w_{9}=$\"*losing*\" does not occur, so $x_{19}=0$. \n","\n","In document $x_2$, \n","- the word $w_1=$\"*the*\" occurs 2 times, so $x_{21}=2$; \n","- the word $w_2=$\"*telescope*\" does not occur, so $x_{22}=0$; \n","- the word $w_{9}=$\"*losing*\" occurs one time, so $x_{29}=1$.\n","\n","Then we can estimate the parameters. According to the above equations, we have\n","\n","$P(c_1)=P(Science)=\\frac{1+\\delta_{11}+\\delta_{21}}{2+2}=\\frac{2}{4}=\\frac{1}{2}$, \n","\n","$P(c_2)=P(Sports)=\\frac{1+\\delta_{12}+\\delta_{22}}{2+2}=\\frac{2}{4}=\\frac{1}{2}$.\n","\n","$P(w_1|c_1)=P(\\text{\"the\"}|Science)=\\frac{1+\\delta_{11}x_{11}+\\delta_{21}x_{21}}{9+\\sum_{s=1}^{9}\\sum_{i=1}^2\\delta_{ij}x_{is}}=\\frac{3}{16}$.\n","<br/><br/>\n","Other word probabilities can be calculated by the same step. Finally, we can estimate all the parameters:<br/><br/>\n","\n","| $P(Science)$ | $P(Sports)$ |\n","| ---: |---: |\n","| $\\frac{1}{2}$ | $\\frac{1}{2}$ |\n","\n","<br/>\n","\n","| word | $P(word|Science)$ | $P(word|Sports)$ |\n","| :--- | ---: |---: |\n","| \"the\" | $\\frac{3}{16}$ | $\\frac{1}{5}$ |\n","| \"telescope\" | $\\frac{1}{8}$ | $\\frac{1}{15}$ |\n","| \"at\" | $\\frac{1}{8}$ | $\\frac{2}{15}$ |\n","| \"observatory\" | $\\frac{1}{8}$ | $\\frac{1}{15}$ |\n","| \"galaxy\" | $\\frac{1}{8}$ | $\\frac{1}{15}$ |\n","| \"scientists\" | $\\frac{1}{8}$ | $\\frac{1}{15}$ |\n","| \"record\" | $\\frac{1}{16}$ | $\\frac{2}{15}$ |\n","| \"place\" | $\\frac{1}{16}$ | $\\frac{2}{15}$ |\n","| \"losing\" | $\\frac{1}{16}$ | $\\frac{2}{15}$ |\n","<br/><br/>\n","\n","For this simple example, we can calculate these probabilities manually. However, when the number of documents and the vocabulary size increase, it costs too much effort to manually estimate all parameters for a naive Bayes text classifier. Fortunately, we can implement a machine learning model to learn the parameters. That is what we will do in the assignment task.\n","\n","***"],"metadata":{"id":"5AIdo3q_IAt8"}},{"cell_type":"markdown","source":["## Prediction for the Testing Data\n","Given estimates of these parameters ($P(c_j)$ and $P(w_k|c_j)$) calculated from training documents, we can calculate the posterior probability of classes for a testing document to perform classification. \n","\n","This follows from an application of Bayesâ€™ rule:\n","\n","$P(y_i=c_j|x_i)=\\frac{P(c_j)P(x_i|c_j)}{P(x_i)}=\\frac{P(c_j)\\prod_{w_k\\in V}P(w_k|c_j)^{x_{ik}}}{\\sum_{q=1}^NP(c_q)\\prod_{w_k\\in V}P(w_k|c_q)^{x_{ik}}}$.\n","\n","Our task is to classify a testing document $x_i$ into a single class, and then the class with the highest probability, i.e.,  $\\arg\\max_jP(y_i=c_j|x_i)$ is selected. "],"metadata":{"id":"ncm_bFPkH9-H"}},{"cell_type":"markdown","source":["# Data Description\n","We use [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) for the text classification task. Specifically, we use the 6 topics as the classes for our text classification task: *sports*, *computer*, *politics*, *religion*, *sales*, and *science*, so the number of classes $N=6$.\n","\n","The vocabulary is already constructed by selecting the most frequent 10,000 words, so the vocabulary size $|V|=10,000$.\n","\n","The documents are split into the training and testing set, which contain 13,570 documents and 3,769 documents, respectively. You will download 4 files: *20ng_train_dataset.npz*, *20ng_test_dataset.npz*, *20ng_train_labels.npy*, *20ng_test_labels.npy*.\n","\n","- *20ng_train_dataset.npz* is a sparse matrix, which we will further convert to a Numpy 2D array with shape $(d, |V|)$, where $d=13,570$. The matrix represents the word frequency, i.e., each element $x_{ik}$ in the matrix is the number of times word $w_k$ occurs in document $x_i$. The matrix corresponds to $x_{ik}$ defined in **Problem Formulation**.\n","\n","- *20ng_test_dataset.npz* is a sparse matrix, which we will further convert to a Numpy 2D array with shape $(d_{test}, |V|)$, where $d_{test}=3,769$. The matrix represents the word frequency, i.e., each element $x_{ik}$ in the matrix is the number of times word $w_k$ occurs in document $x_i$. The matrix corresponds to $x_{ik}$ defined in **Problem Formulation**.\n","\n","- *20ng_train_labels.npy* is a Numpy 1D array with shape $(d, )$. The array represents the class labels for the documents, i.e., each element is an integer from $\\{0,1,2,3,4,5\\}$, representing the index of the class.\n","\n","- *20ng_test_labels.npy* is a Numpy 1D array with shape $(d_{test}, )$. The array represents the class labels for the documents, i.e., each element is an integer from $\\{0,1,2,3,4,5\\}$, representing the index of the class."],"metadata":{"id":"GzL0XiTiH4YL"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"e4LmmLcIHqFR"}},{"cell_type":"markdown","source":["## Task 0: Download the Dataset\n","Run this code cell with these two !wget Linux commands to connect and login to the COMP2211 course server and download the datasets. Modify the *--user=username* portion with your CSD username. The output will prompt you to enter your CSD password and will not be saved in Google Colab. **Remember to comment out this section before downloading it as .py for ZINC submission**."],"metadata":{"id":"uFOU4LbPHx9M"}},{"cell_type":"code","source":["# !wget --user=jhongae --password=KX303006165964 https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_train_dataset.npz\n","# !wget --user=jhongae --password=KX303006165964 https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_test_dataset.npz\n","# !wget --user=jhongae --password=KX303006165964 https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_train_labels.npy\n","# !wget --user=jhongae --password=KX303006165964 https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_test_labels.npy"],"metadata":{"id":"kIXJa3ZSbrPh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664954908395,"user_tz":-480,"elapsed":7669,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}},"outputId":"be9a17c5-2711-41a6-88cb-3878220102b3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-05 07:28:20--  https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_train_dataset.npz\n","Resolving course.cse.ust.hk (course.cse.ust.hk)... 143.89.41.176\n","Connecting to course.cse.ust.hk (course.cse.ust.hk)|143.89.41.176|:443... connected.\n","HTTP request sent, awaiting response... 401 Unauthorized\n","Authentication selected: Basic realm=\"Enter Your CSD PC/Unix Password\"\n","Reusing existing connection to course.cse.ust.hk:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3253235 (3.1M)\n","Saving to: â€˜20ng_train_dataset.npzâ€™\n","\n","20ng_train_dataset. 100%[===================>]   3.10M  2.29MB/s    in 1.4s    \n","\n","2022-10-05 07:28:23 (2.29 MB/s) - â€˜20ng_train_dataset.npzâ€™ saved [3253235/3253235]\n","\n","--2022-10-05 07:28:23--  https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_test_dataset.npz\n","Resolving course.cse.ust.hk (course.cse.ust.hk)... 143.89.41.176\n","Connecting to course.cse.ust.hk (course.cse.ust.hk)|143.89.41.176|:443... connected.\n","HTTP request sent, awaiting response... 401 Unauthorized\n","Authentication selected: Basic realm=\"Enter Your CSD PC/Unix Password\"\n","Reusing existing connection to course.cse.ust.hk:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 811911 (793K)\n","Saving to: â€˜20ng_test_dataset.npzâ€™\n","\n","20ng_test_dataset.n 100%[===================>] 792.88K   817KB/s    in 1.0s    \n","\n","2022-10-05 07:28:25 (817 KB/s) - â€˜20ng_test_dataset.npzâ€™ saved [811911/811911]\n","\n","--2022-10-05 07:28:25--  https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_train_labels.npy\n","Resolving course.cse.ust.hk (course.cse.ust.hk)... 143.89.41.176\n","Connecting to course.cse.ust.hk (course.cse.ust.hk)|143.89.41.176|:443... connected.\n","HTTP request sent, awaiting response... 401 Unauthorized\n","Authentication selected: Basic realm=\"Enter Your CSD PC/Unix Password\"\n","Reusing existing connection to course.cse.ust.hk:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 54408 (53K)\n","Saving to: â€˜20ng_train_labels.npyâ€™\n","\n","20ng_train_labels.n 100%[===================>]  53.13K   139KB/s    in 0.4s    \n","\n","2022-10-05 07:28:26 (139 KB/s) - â€˜20ng_train_labels.npyâ€™ saved [54408/54408]\n","\n","--2022-10-05 07:28:26--  https://course.cse.ust.hk/comp2211/assignments/pa1/data/20ng_test_labels.npy\n","Resolving course.cse.ust.hk (course.cse.ust.hk)... 143.89.41.176\n","Connecting to course.cse.ust.hk (course.cse.ust.hk)|143.89.41.176|:443... connected.\n","HTTP request sent, awaiting response... 401 Unauthorized\n","Authentication selected: Basic realm=\"Enter Your CSD PC/Unix Password\"\n","Reusing existing connection to course.cse.ust.hk:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15204 (15K)\n","Saving to: â€˜20ng_test_labels.npyâ€™\n","\n","20ng_test_labels.np 100%[===================>]  14.85K  77.4KB/s    in 0.2s    \n","\n","2022-10-05 07:28:28 (77.4 KB/s) - â€˜20ng_test_labels.npyâ€™ saved [15204/15204]\n","\n"]}]},{"cell_type":"markdown","source":["## Import Numpy (Optional Task: Import Modules from Python Standard Library)\n","\n","Run this code cell to import Numpy. You may also import other modules, as long as they are part of the Python Standard Library. https://docs.python.org/3/library/\n","\n","You are **NOT** allowed to import any other external libraries (e.g., sklearn) except for the library specified in **Optional Task: Test Run**."],"metadata":{"id":"4B0ERj1OccHt"}},{"cell_type":"code","source":["import numpy as np\n"],"metadata":{"id":"HOecfRzAddGM","executionInfo":{"status":"ok","timestamp":1664954908902,"user_tz":-480,"elapsed":509,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Task 1: Naive Bayes Text Classifier\n","Since we don't expect you to have Object-Oriented Programming background, we have implemented the class-related functions for you. \n","\n","These are the class attributes of *NaiveBayesClassifer*. They can be accessed within all of the functions defined inside *NaiveBayesClassifer*: \\\n","*self.train_dataset*: The word frequency of the training dataset with shape (*num_train_documents*, *vocab_size*). \\\n","*self.test_dataset*: The word frequency of the testing dataset with shape (*num_test_documents*, *vocab_size*). \\\n","*self.train_labels*: The class labels of the training dataset with shape (*num_train_documents*, ). \\\n","*self.test_labels*: The class labels of the testing dataset with shape (*num_test_documents*, ). \\\n","\\\n","Hint: Use Numpy broadcasting for efficient and concise code. \\\n","https://numpy.org/doc/stable/user/basics.broadcasting.html \\\n","\n","Common Numpy array shape manipulation functions for your reference: \\\n","https://numpy.org/doc/stable/reference/generated/numpy.reshape.html \\\n","https://numpy.org/doc/stable/reference/generated/numpy.transpose.html \\\n","https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html \\\n","https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html \\\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.zeros.html \\\n","https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html \\\n","https://numpy.org/doc/stable/reference/generated/numpy.ones.html \\\n","https://numpy.org/doc/stable/reference/generated/numpy.ones_like.html"],"metadata":{"id":"zhzO-04Udb60"}},{"cell_type":"markdown","source":["## Task 1.1: Build Training Delta Matrix\n","Implement the function *build_training_delta_matrix(self)*.\n","\n","<mark> Return a Numpy 2D array with shape *(num_train_documents, num_classes)*</mark>, an indication matrix, corresponding to $\\delta_{ij}$ in **Problem Formulation**:\n","\n","$\\delta_{ij}=1$ when $y_i=c_j$ and $\\delta_{ij}=0$ otherwise.\n","\n","Hint 1: *self.train_labels* has shape *(num_train_documents, )*.\n","\n","Hint 2: *num_classes* is the maximum value + 1 in *self.train_labels* since the class index starts from $0$.\n","\n","Hint 3: You may use Numpy's *shape()*, *amax()*, *arange()*, *reshape()* functions.\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.shape.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.amax.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.arange.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html"],"metadata":{"id":"xO9CSy9ml7FN"}},{"cell_type":"markdown","source":["## Task 1.2: Estimate Class Probabilities\n","Implement the function *estimate_class_probabilities(self)*.\n","\n","<mark>Return a Numpy 2D array with shape *(1, num_classes)*</mark>, containing the probabilities of all classes:\n","\n","$P(c_j)=\\frac{1+\\sum_{i=1}^d\\delta_{ij}}{N+d}$.\n","\n","Hint 1: You may call *build_training_delta_matrix()* to reuse your function from Task 1.1.\n","\n","Hint 2: You may use Numpy's *sum()* function.\n","\n","Hint 3: Specify the correct values for *axis* and *keepdims* in *sum()* for Numpy broadcasting.\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.sum.html"],"metadata":{"id":"9ujV0DN3rGRd"}},{"cell_type":"markdown","source":["## Task 1.3: Estimate Word Probabilities\n","Implement the function *estimate_word_probabilities(self)*.\n","\n","<mark>Return a Numpy 2D array with shape *(vocab_size, num_classes)*</mark>, containing the probabilities of all words given each class:\n","\n","$P(w_k|c_j)=\\frac{1+\\sum_{i=1}^d\\delta_{ij}x_{ik}}{|V|+\\sum_{s=1}^{|V|}\\sum_{i=1}^d\\delta_{ij}x_{is}}$.\n","\n","Hint 1: You may call *build_training_delta_matrix()* to reuse your function from Task 1.1.\n","\n","Hint 2:  *self.train_dataset* has shape *(num_train_documents, vocab_size)*.\n","\n","Hint 3: You may use Numpy's *transpose(), dot(), sum()* functions.\n","\n","Hint 4: Specify the correct values for *axis* and *keepdims* in *sum()* for Numpy broadcasting.\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.sum.html"],"metadata":{"id":"0GBsDc5JuNH2"}},{"cell_type":"markdown","source":["## Task 1.4: Predict Label for Testing Documents\n","Implement the function *predict(self)*. For the sake of simplicity, you can assume that ties will never occur.\n","\n","<mark>Return the predicted labels (integer value from $0$ to *num_classes*$-1$) of *self.test_dataset* as a Numpy 1D array with shape *(num_test_documents, )*</mark>.\n","\n","Hint 1: You may call *estimate_class_probabilities()* and *estimate_word_probabilities()* to reuse your functions from Task 1.2 and 1.3.\n","\n","Hint 2: *self.test_dataset* has shape *(num_test_documents, vocab_size)*.\n","\n","Hint 3: Recall that we select the class with the highest posterior probability to classify a testing document:\n","\n","$P(y_i=c_j|x_i)=\\frac{P(c_j)P(x_i|c_j)}{P(x_i|\\theta_{c_j})}=\\frac{P(c_j)\\prod_{w_k\\in V}P(w_k|c_j)^{x_{ik}}}{\\sum_{q=1}^NP(c_q)\\prod_{w_k\\in V}P(w_k|c_q)^{x_{ik}}}$.\n","\n","In practical implementation, it is not necessary to compute the posterior probabilities of all classes for all testing documents. For a specific testing document $x_i$, the denominator $\\sum_{q=1}^NP(c_q)\\prod_{w_k\\in V}P(w_k|c_q)^{x_{ik}}$ is the same for all $c_j$, so we only need to compare the numerators $P(c_j)\\prod_{w_k\\in V}P(w_k|c_j)^{x_{ik}}$, which can be further simplified by comparing the *log* of the numerators: $\\log(P(c_j)\\prod_{w_k\\in V}P(w_k|c_j)^{x_{ik}})=\\log(P(c_j))+\\sum_{w_k\\in V}x_{ik}\\log(P(w_k|c_j))$.\n","\n","Hint 4: You may use Numpy's *log(),dot(),argmax()* functions.\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.log.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.argmax.html"],"metadata":{"id":"E18S418Kz2C8"}},{"cell_type":"markdown","source":["## Tasks 1.1 - 1.4 Code Cell:"],"metadata":{"id":"xx-sofqWfxgJ"}},{"cell_type":"code","source":["class NaiveBayesClassifier:\n","  def __init__(self, train_dataset, test_dataset, train_labels, test_labels):\n","    self.train_dataset = train_dataset\n","    self.test_dataset = test_dataset\n","    self.train_labels = train_labels\n","    self.test_labels = test_labels\n","\n","  def build_training_delta_matrix(self):\n","      #Length of Train Label Dataset\n","      train_label_size = len(self.train_labels)\n","      \n","      # Num of Classes\n","      num_class = self.train_labels.max() + 1\n","      \n","      # Initialize Matrix with size (num_train_documents, num_classes) with zeros\n","      deltas = np.zeros((train_label_size, num_class))\n","      \n","      # One hot encoding\n","      deltas[np.arange(self.train_labels.size), self.train_labels] = 1\n","      \n","      return deltas\n","\n","  def estimate_class_probabilities(self):\n","      # Num of Classes\n","      num_class = self.train_labels.max() + 1\n","\n","      # Num of Docs \n","      num_train_doc = len(self.train_labels)\n","      \n","      class_prob = np.zeros((1, num_class))\n","      deltas = self.build_training_delta_matrix()\n","      prob = (1 + deltas.sum(axis = 0)) / (num_class + num_train_doc)\n","      class_prob[:, np.arange(6)] = prob\n","      \n","      return class_prob\n","\n","  def estimate_word_probabilities(self):\n","\n","      # Num of Classes\n","      num_class = self.train_labels.max() + 1\n","\n","      # Vocab V\n","      v = np.shape(self.train_dataset)[1]\n","\n","      # Word_prob:\n","      word_prob = np.zeros((v, num_class))\n","\n","      deltas = self.build_training_delta_matrix()\n","      temp_mat = np.dot(self.train_dataset.transpose(), deltas.astype(bool))\n","\n","      sum_mat = temp_mat.sum(axis = 0)\n","\n","      word_prob = (1 + temp_mat) / (v + sum_mat)\n","      \n","      return word_prob\n","\n","  def predict(self):\n","      #TODO 1D array with shape (num_test_documents, )\n","\n","      # Num of Classes\n","      num_class = self.train_labels.max() + 1\n","\n","      # Num documents\n","      d = np.shape(self.test_dataset)[0]\n","\n","      # Call Class & Word Prob\n","      class_prob = self.estimate_class_probabilities()\n","      word_prob = self.estimate_word_probabilities()\n","\n","      word_prob[word_prob > 0] = np.log(word_prob[word_prob > 0])\n","      temp_mat = self.test_dataset.dot(word_prob)                # temp_mat shape(3769, 6) = (d, N)\n","\n","      # Reshape class prob to (d, N)\n","      class_prob_shaped = np.tile(np.log(class_prob), (d, 1))\n","\n","      array = class_prob_shaped + temp_mat\n","\n","      test_predict = np.argmax(array, axis = 1)\n","      \n","      return test_predict\n","\n"],"metadata":{"id":"h3Km3HlOgBEZ","executionInfo":{"status":"ok","timestamp":1664954908903,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Task 2: Evaluation Metrics\n","\n","To evaluate the Naive Bayes text classifier, we use *precision*, *recall*, *Micro-$F_1$ score*, and *Macro-$F_1$ score* as the evaluation metrics. To compute these metrics more conveniently, we need to get the confusion matrix first."],"metadata":{"id":"wVvsX53Jf_-O"}},{"cell_type":"markdown","source":["## Task 2.1: Confusion Matrix\n","\n","A confusion matrix is a table that summarizes the predictions and actual labels of a binary classification. Since our task is multi-class classification, we have a confusion matrix for each class. For each class $c_j$, denote $TP_j$, $TN_j$, $FP_j$, $FN_j$, as the instance numbers of true-positive, true-negative, false-positive and false negative, whose definitions are as follows:\n","- True positive: A test result that correctly indicates the presence of a condition or characteristic.\n","- True negative: A test result that correctly indicates the absence of a condition or characteristic.\n","- False positive: A test result that wrongly indicates that a particular condition or attribute is present.\n","- False negative: A test result that wrongly indicates that a particular condition or attribute is absent.\n","\n","Please refer to the [Wikipedia page](https://en.wikipedia.org/wiki/Confusion_matrix) for details if interested. \n","\n","For our implementation of *generate_confusion_matrix(test_predict, test_labels)*, we don't need to specifically create a matrix or table. It is sufficient to only calculate and <mark>return the four Numpy arrays of *TP*, *TN*, *FP*, and *FN*. Each array has the shape *(num_classes, )*</mark>.\n","\n","Hint 1: Both *test_predict* and *test_labels* are Numpy 1D arrays with shape (*num_test_documents*, ).\n","\n","Hint 2: *num_classes* is the maximum number + 1 in *test_labels* since the class index starts from $0$.\n","\n","Hint 3: You may use Numpy's *arange()*, *reshape()*  function.\n","\n","Hint 4: Keep the index of the classes consistent between the returned arrays and *test_labels*.\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.arange.html\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html"],"metadata":{"id":"mxmFHGMBIUzX"}},{"cell_type":"markdown","source":["## Task 2.2: Precision\n","Implement *calculate_precision(test_predict, test_labels)* to return the precision score.\n","\n","In terms of the confusion matrix, the precision formula is:\n","\n","$P=\\frac{\\sum_{i=1}^MTP_i}{\\sum_{i=1}^M(TP_i+FP_i)}$.\n","\n","Hint: You may call the *generate_confusion_matrix(test_predict, test_labels)* function from Task 2.1."],"metadata":{"id":"Aq9FV24pIRqT"}},{"cell_type":"markdown","source":["## Task 2.3: Recall\n","Implement *calculate_recall(test_predict, test_labels)* to return the recall score.\n","\n","In terms of the confusion matrix, the recall formula is:\n","\n","$R=\\frac{\\sum_{i=1}^MTP_i}{\\sum_{i=1}^M(TP_i+FN_i)}$.\n","\n","Hint: You may call the *generate_confusion_matrix(test_predict, test_labels)* function from Task 2.1."],"metadata":{"id":"js-TAL1lIOfW"}},{"cell_type":"markdown","source":["## Task 2.4: Micro-$F_1$ Score\n","Implement *calculate_micro_f1(test_predict, test_labels)* to return the micro-$F_1$ score.\n","\n","In terms of the confusion matrix, the micro-$F_1$ score formula is:\n","\n","$Micro-F_1=\\frac{2PR}{P+R}$.\n","\n","Hint: You may call the *calculate_precision(test_predict, test_labels)* and *calculate_recall(test_predict, test_labels)* functions from Task 2.2 and 2.3."],"metadata":{"id":"M2mM8CaPILfu"}},{"cell_type":"markdown","source":["## Task 2.5: Macro-$F_1$ Score\n","Implement *calculate_macro_f1(test_predict, test_labels)* to return the macro-$F_1$ score.\n","\n","The macro-$F_1$ score formula is:\n","\n","$Macro-F_1=\\frac{1}{M}\\sum_{i=1}^M\\frac{2P_iR_i}{P_i+R_i}$,\n","\n","where $M$ is the number of classes. $P_i$ and $R_i$ are the *precision* and *recall* for class $c_i$: $P_i=\\frac{TP_i}{TP_i+FP_i}$, $R_i=\\frac{TP_i}{TP_i+FN_i}$.\n","\n","Hint 1: You may call the *generate_confusion_matrix(test_predict, test_labels)* function from Task 2.1.\n","\n","Hint 2: You may use Numpy's *average()* function.\n","\n","https://numpy.org/doc/stable/reference/generated/numpy.average.html"],"metadata":{"id":"vCNOKYCjp7Az"}},{"cell_type":"markdown","source":["## Tasks 2.1 - 2.5 Code Cell:"],"metadata":{"id":"MeD_vvS28BwO"}},{"cell_type":"code","source":["def generate_confusion_matrix(test_predict, test_labels):\n","\n","    # Num Classes\n","    num_class = test_labels.max() + 1\n","\n","    # Num Documents\n","    num_doc = len(test_labels)\n","\n","    # Initialize Empty TP TN FP FN -- shape(num_class, )\n","    TP = np.zeros(num_class, dtype = int)\n","    TN = np.zeros(num_class, dtype = int)\n","    FP = np.zeros(num_class, dtype = int)\n","    FN = np.zeros(num_class, dtype = int)\n","\n","    # Iterate over classes\n","    for i in range(num_class):\n","\n","        # Iterate over docs\n","        for j in range(num_doc):\n","            if ((test_labels[j] == i) and (test_predict[j] == i)):\n","                TP[i] += 1\n","\n","            if ((test_labels[j] != i) and (test_predict[j] != i)):\n","                TN[i] += 1\n","\n","            if ((test_labels[j] != i) and (test_predict[j] == i)):\n","                FP[i] += 1\n","\n","            if ((test_labels[j] == i) and (test_predict[j] != i)):\n","                FN[i] += 1\n","                \n","    return TP, TN, FP, FN\n","\n","def calculate_precision(test_predict, test_labels):\n","\n","    TP, TN, FP, FN = generate_confusion_matrix(test_predict, test_labels)\n","    \n","    precision = TP.sum() / (TP.sum() + FP.sum())\n","\n","    return precision\n","\n","def calculate_recall(test_predict, test_labels):\n","\n","    TP, TN, FP, FN = generate_confusion_matrix(test_predict, test_labels)\n","    \n","    recall = TP.sum() / (TP.sum() + FN.sum())\n","\n","    return recall\n","\n","def calculate_micro_f1(test_predict, test_labels):\n","\n","    P = calculate_precision(test_predict, test_labels)\n","    R = calculate_recall(test_predict, test_labels)\n","\n","    micro_f1 = (2 * P * R) / (P + R)\n","    return micro_f1\n","\n","def calculate_macro_f1(test_predict, test_labels):\n","\n","    # Num Classes\n","    num_class = test_labels.max() + 1\n","\n","    TP, TN, FP, FN = generate_confusion_matrix(test_predict, test_labels)\n","    \n","    P = TP / (TP + FP)\n","    R = TP / (TP + FN)\n","\n","    macro_f1 = (((2 * P * R) / (P + R)).sum()) / num_class\n","\n","    return macro_f1"],"metadata":{"id":"kMN8ZPen8JKW","executionInfo":{"status":"ok","timestamp":1664983371636,"user_tz":-480,"elapsed":319,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Optional Task: Test Run\n","Use all the previously defined functions in Tasks 1 and 2 to perform the Naive Bayes text classifier on our 20 Newsgroups dataset. Feel free to modify this code cell for your own testing and debugging purposes, which will not be graded."],"metadata":{"id":"9DX2HK_KDwW1"}},{"cell_type":"code","source":["import scipy.sparse as sparse\n","# import numpy as np\n","\n","if __name__ == '__main__':\n","  train_dataset = sparse.load_npz(\"20ng_train_dataset.npz\")\n","  test_dataset = sparse.load_npz(\"20ng_test_dataset.npz\")\n","  train_dataset = train_dataset.toarray()\n","  test_dataset = test_dataset.toarray()\n","  train_labels = np.load(\"20ng_train_labels.npy\")\n","  test_labels = np.load(\"20ng_test_labels.npy\")\n","\n","# print(train_dataset.shape)\n","# print(len(train_labels))\n","# print(test_dataset.shape)\n","# print(test_labels.shape)\n","\n"],"metadata":{"id":"X6ZDDEPLEGgh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664954914178,"user_tz":-480,"elapsed":5277,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}},"outputId":"d7af1252-561b-4235-b44c-687d27886033"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(13570, 100000)\n","13570\n","(3769, 100000)\n","(3769,)\n"]}]},{"cell_type":"code","source":["# class1 = NaiveBayesClassifier(train_dataset, test_dataset, train_labels, test_labels)\n","# # # # #deltas = class1.build_training_delta_matrix()\n","# pre = class1.predict()\n","\n","# # generate_confusion_matrix(pre, test_labels)\n","# #print(calculate_recall(pre, test_labels))\n","# print(calculate_precision(pre, test_labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLsL597i_G1N","executionInfo":{"status":"ok","timestamp":1664956913680,"user_tz":-480,"elapsed":5471,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}},"outputId":"1ce299d0-eab5-4420-daf2-d4cc7dc6479d"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.93727811 0.94       0.9184466  0.97077244 0.96396396 0.86446886]\n"]}]},{"cell_type":"code","source":["# import scipy.sparse as sparse\n","\n","# train_dataset = sparse.load_npz(\"20ng_train_dataset.npz\")\n","# test_dataset = sparse.load_npz(\"20ng_test_dataset.npz\")\n","# train_dataset = train_dataset.toarray()\n","# test_dataset = test_dataset.toarray()\n","# train_labels = np.load(\"20ng_train_labels.npy\")\n","# test_labels = np.load(\"20ng_test_labels.npy\")\n","\n","# print(train_dataset.shape)\n","# print(len(train_labels))\n","# print(test_dataset.shape)\n","# print(test_labels.shape)"],"metadata":{"id":"oMG_ImFJSUaa","executionInfo":{"status":"ok","timestamp":1664954919991,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# print(np.shape(train_dataset)[1])"],"metadata":{"id":"NANvDvh2M8je","executionInfo":{"status":"ok","timestamp":1664954919992,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# class1 = NaiveBayesClassifier(train_dataset, test_dataset, train_labels, test_labels)\n","# delta = class1.build_training_delta_matrix()\n","# t_train = train_dataset.transpose()\n","# print(delta.dtype)\n","\n","# print(t_train.shape)\n","# print(delta.shape)\n","# print(class1.estimate_word_probabilities())\n","\n","#print(temp_mat)"],"metadata":{"id":"b5JTOGk5NfP5","executionInfo":{"status":"ok","timestamp":1664954919992,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# print(train_dataset.shape)\n","# print(len(train_labels))\n","# print(test_dataset.shape)\n","# print(test_labels.shape)\n","\n","#class1 = NaiveBayesClassifier(train_dataset, test_dataset, train_labels, test_labels)\n","# deltas = class1.build_training_delta_matrix()\n","# print(deltas.dtype)\n","# print(deltas)\n","\n","# b = class1.estimate_class_probabilities()\n","# print(b)\n","\n","# c = class1.estimate_word_probabilities()\n","# print(c)"],"metadata":{"id":"0t9itncOScab","executionInfo":{"status":"ok","timestamp":1664954919992,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jerry H.","userId":"01181828413146773260"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# References\n","Kamal Nigam, Andrew McCallum, and Tom Mitchell. 2006. Semi-supervised text classification using EM. Semi-Supervised Learning (2006), 33â€“56."],"metadata":{"id":"LL7aL_Q_DZFM"}}]}